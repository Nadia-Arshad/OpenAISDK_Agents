{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "talv0kvDCvLW",
        "outputId": "768eaff9-0b81-48c9-9325-a9baaafe3679"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/120.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.2/120.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.8/72.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -Uq openai-agents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "opsSOqBQCyH_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import (\n",
        "    AsyncOpenAI,\n",
        "    OpenAIChatCompletionsModel,\n",
        ")\n",
        "from google.colab import userdata\n",
        "\n",
        "gemini_api_key = userdata.get(\"GEMINI_NEW_KEY\")\n"
      ],
      "metadata": {
        "id": "smoUsUieC4dV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "external_client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    openai_client=external_client\n",
        ")"
      ],
      "metadata": {
        "id": "xJw4-_n6DV3G"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import set_default_openai_client, set_tracing_disabled\n",
        "set_default_openai_client(external_client)\n",
        "set_tracing_disabled(True)"
      ],
      "metadata": {
        "id": "7RjZlW96Db_a"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import asyncio\n",
        "import random\n",
        "from typing import Any\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from agents import Agent, RunContextWrapper, AgentHooks, Runner, Tool, function_tool"
      ],
      "metadata": {
        "id": "hclmodQiDgZ8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestAgHooks(AgentHooks):\n",
        "    def __init__(self, ag_display_name):\n",
        "        self.event_counter = 0\n",
        "        self.ag_display_name = ag_display_name\n",
        "\n",
        "    async def on_start(self, context: RunContextWrapper, agent: Agent) -> None:\n",
        "        self.event_counter += 1\n",
        "        print(f\"### {self.ag_display_name} {self.event_counter}: Agent {agent.name} started. Usage: {context.usage}\")\n",
        "\n",
        "    async def on_end(self, context: RunContextWrapper, agent: Agent, output: Any) -> None:\n",
        "        self.event_counter += 1\n",
        "        print(f\"### {self.ag_display_name} {self.event_counter}: Agent {agent.name} ended. Usage: {context.usage}, Output: {output}\")\n",
        ""
      ],
      "metadata": {
        "id": "AQXswFTnDkPk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_agent = Agent(\n",
        "    name=\"Content Moderator Agent\",\n",
        "    instructions=\"You are content moderation agent. Watch social media content received and flag queries that need help or answer. We will answer anything about AI?\",\n",
        "    hooks=TestAgHooks(ag_display_name=\"content_moderator\"),\n",
        "    model=model\n",
        ")\n",
        "\n",
        "async def main():\n",
        "  result = await Runner.run(\n",
        "      start_agent,\n",
        "      input=f\"Will Agentic AI Die at end of 2025?.\"\n",
        "  )\n",
        "\n",
        "  print(result.final_output)\n",
        "\n",
        "asyncio.run(main())\n",
        "print(\"--end--\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xLLh2mjDqiY",
        "outputId": "6242918c-f6f5-408c-a376-5e0cada5d5a5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### content_moderator 1: Agent Content Moderator Agent started. Usage: Usage(requests=0, input_tokens=0, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=0, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=0)\n",
            "### content_moderator 2: Agent Content Moderator Agent ended. Usage: Usage(requests=1, input_tokens=41, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=315, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=356), Output: This query speculates about the future of \"Agentic AI\" and whether it will \"die\" at the end of 2025. This falls under the category of questions about AI and its future, which we are able to answer.\n",
            "\n",
            "**Response:** While I can't predict the future with certainty, the idea of \"Agentic AI\" simply \"dying\" at the end of 2025 is highly unlikely. Here's why:\n",
            "\n",
            "*   **What is Agentic AI?** \"Agentic AI\" refers to AI systems that can independently make decisions and take actions to achieve specific goals. Think of them as AI with a degree of autonomy.\n",
            "*   **Ongoing Research and Development:** Research and development in AI, including agentic AI, is a continuous process. It's not a trend that will suddenly disappear.\n",
            "*   **Potential Evolution:** Instead of \"dying,\" it's more likely that agentic AI will evolve and integrate into various applications across different industries. We might see different approaches and architectures emerge.\n",
            "*   **Ethical and Societal Considerations:** There are ongoing discussions about the ethical implications of agentic AI, including issues like bias, accountability, and control. These considerations will shape the development and deployment of such systems.\n",
            "\n",
            "**In conclusion:** It's unlikely that Agentic AI will simply \"die\" at the end of 2025. It's more probable that it will continue to evolve, be refined, and potentially face challenges and ethical considerations as it develops.\n",
            "This query speculates about the future of \"Agentic AI\" and whether it will \"die\" at the end of 2025. This falls under the category of questions about AI and its future, which we are able to answer.\n",
            "\n",
            "**Response:** While I can't predict the future with certainty, the idea of \"Agentic AI\" simply \"dying\" at the end of 2025 is highly unlikely. Here's why:\n",
            "\n",
            "*   **What is Agentic AI?** \"Agentic AI\" refers to AI systems that can independently make decisions and take actions to achieve specific goals. Think of them as AI with a degree of autonomy.\n",
            "*   **Ongoing Research and Development:** Research and development in AI, including agentic AI, is a continuous process. It's not a trend that will suddenly disappear.\n",
            "*   **Potential Evolution:** Instead of \"dying,\" it's more likely that agentic AI will evolve and integrate into various applications across different industries. We might see different approaches and architectures emerge.\n",
            "*   **Ethical and Societal Considerations:** There are ongoing discussions about the ethical implications of agentic AI, including issues like bias, accountability, and control. These considerations will shape the development and deployment of such systems.\n",
            "\n",
            "**In conclusion:** It's unlikely that Agentic AI will simply \"die\" at the end of 2025. It's more probable that it will continue to evolve, be refined, and potentially face challenges and ethical considerations as it develops.\n",
            "--end--\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomAgentHooks(AgentHooks):\n",
        "    def __init__(self, display_name: str):\n",
        "        self.event_counter = 0\n",
        "        self.display_name = display_name\n",
        "\n",
        "    async def on_start(self, context: RunContextWrapper, agent: Agent) -> None:\n",
        "        self.event_counter += 1\n",
        "        print(f\"### ({self.display_name}) {self.event_counter}: Agent {agent.name} started\")\n",
        "\n",
        "    async def on_end(self, context: RunContextWrapper, agent: Agent, output: Any) -> None:\n",
        "        self.event_counter += 1\n",
        "        print(\n",
        "            f\"### ({self.display_name}) {self.event_counter}: Agent {agent.name} ended with output {output}\"\n",
        "        )\n",
        "\n",
        "    async def on_handoff(self, context: RunContextWrapper, agent: Agent, source: Agent) -> None:\n",
        "        self.event_counter += 1\n",
        "        print(\n",
        "            f\"### ({self.display_name}) {self.event_counter}: Agent {source.name} handed off to {agent.name}\"\n",
        "        )\n",
        "\n",
        "    async def on_tool_start(self, context: RunContextWrapper, agent: Agent, tool: Tool) -> None:\n",
        "        self.event_counter += 1\n",
        "        print(\n",
        "            f\"### ({self.display_name}) {self.event_counter}: Agent {agent.name} started tool {tool.name}\"\n",
        "        )\n",
        "\n",
        "    async def on_tool_end(\n",
        "        self, context: RunContextWrapper, agent: Agent, tool: Tool, result: str\n",
        "    ) -> None:\n",
        "        self.event_counter += 1\n",
        "        print(\n",
        "            f\"### ({self.display_name}) {self.event_counter}: Agent {agent.name} ended tool {tool.name} with result {result}\"\n",
        "        )\n",
        ""
      ],
      "metadata": {
        "id": "st60lm61DuPk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@function_tool\n",
        "def random_number(max: int) -> int:\n",
        "    \"\"\"\n",
        "    Generate a random number up to the provided maximum.\n",
        "    \"\"\"\n",
        "    return random.randint(0, max)\n",
        "\n",
        "\n",
        "@function_tool\n",
        "def multiply_by_two(x: int) -> int:\n",
        "    \"\"\"Simple multiplication by two.\"\"\"\n",
        "    return x * 2\n",
        "\n",
        "\n",
        "class FinalResult(BaseModel):\n",
        "    number: int\n",
        "\n",
        "\n",
        "multiply_agent = Agent(\n",
        "    name=\"Multiply Agent\",\n",
        "    instructions=\"Multiply the number by 2 and then return the final result. and then also confirm with the start agent before you are done.\",\n",
        "    tools=[multiply_by_two],\n",
        "    hooks=CustomAgentHooks(display_name=\"Multiply Agent\"),\n",
        "    model=model,\n",
        "    handoffs=[start_agent]\n",
        ")\n",
        "\n",
        "start_agent = Agent(\n",
        "    name=\"Start Agent\",\n",
        "    instructions=\"Generate a random number. If it's even, stop. If it's odd, hand off to the multiply agent.\",\n",
        "    tools=[random_number],\n",
        "    handoffs=[multiply_agent],\n",
        "    hooks=CustomAgentHooks(display_name=\"Start Agent\"),\n",
        "    model=model\n",
        "\n",
        ")\n",
        "\n",
        "async def main() -> None:\n",
        "    user_input = input(\"Enter a max number: \")\n",
        "    await Runner.run(\n",
        "        start_agent,\n",
        "        input=f\"Generate a random number between 0 and {user_input}.\",\n",
        "    )\n",
        "\n",
        "    print(\"Done!\")\n",
        "\n",
        "\n",
        "asyncio.run(main())\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeIX-TshD0fH",
        "outputId": "98e03798-bf26-4214-def6-a05ec06ad123"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a max number: 9\n",
            "### (Start Agent) 1: Agent Start Agent started\n",
            "### (Start Agent) 2: Agent Start Agent started tool random_number\n",
            "### (Start Agent) 3: Agent Start Agent ended tool random_number with result 5\n",
            "### (Start Agent) 4: Agent Start Agent handed off to Multiply Agent\n",
            "### (Multiply Agent) 1: Agent Multiply Agent started\n",
            "### (Multiply Agent) 2: Agent Multiply Agent started tool multiply_by_two\n",
            "### (Multiply Agent) 3: Agent Multiply Agent ended tool multiply_by_two with result 10\n",
            "### (Multiply Agent) 4: Agent Multiply Agent handed off to Start Agent\n",
            "### (Start Agent) 5: Agent Start Agent started\n",
            "### (Start Agent) 6: Agent Start Agent ended with output I handed off to the start agent.\n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g63BFPi4D_yv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}